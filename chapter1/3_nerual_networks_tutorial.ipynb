{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用torch.nn包来构建神经网络。\n",
    "\n",
    "上一讲已经讲过了autograd，nn包依赖autograd包来定义模型并求导。 一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。\n",
    "\n",
    "例如：\n",
    "它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。\n",
    "\n",
    "神经网络的典型训练过程如下：\n",
    "\n",
    "定义包含一些可学习的参数(或者叫权重)神经网络模型；\n",
    "在数据集上迭代；\n",
    "通过神经网络处理输入；\n",
    "计算损失(输出结果和正确值的差值大小)；\n",
    "将梯度反向传播回网络的参数；\n",
    "更新网络的参数，主要使用如下简单的更新原则：\n",
    "weight = weight - learning_rate * gradient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        #1 1 input image channel,6 output channels,5X5 square convolution\n",
    "        #kernel\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        #an affine operation:y = wx+b\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #MAX pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "[Parameter containing:\n",
      "tensor([[[[-0.1777,  0.1334,  0.1928,  0.1886, -0.0259],\n",
      "          [-0.0640,  0.1965, -0.0622,  0.0577,  0.0441],\n",
      "          [-0.1418, -0.1732, -0.0646, -0.0613,  0.0869],\n",
      "          [-0.0068,  0.1365,  0.1388,  0.0300,  0.0525],\n",
      "          [ 0.0023,  0.0736, -0.1550, -0.1168,  0.0338]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340,  0.0732,  0.0102, -0.1594, -0.0181],\n",
      "          [-0.1757, -0.0435,  0.1386,  0.0353,  0.0996],\n",
      "          [ 0.0406, -0.1572, -0.0910, -0.0728,  0.1855],\n",
      "          [-0.1952,  0.0627, -0.1137, -0.0414, -0.0563],\n",
      "          [-0.0475, -0.1039,  0.1167,  0.1392,  0.1661]]],\n",
      "\n",
      "\n",
      "        [[[-0.0891,  0.0264,  0.0981,  0.1642,  0.1386],\n",
      "          [ 0.1290,  0.1251, -0.1723,  0.0739, -0.1682],\n",
      "          [-0.1590,  0.0480,  0.1336,  0.0448,  0.1866],\n",
      "          [-0.1877,  0.0865,  0.1541,  0.0366, -0.0533],\n",
      "          [-0.1393, -0.0767, -0.0869,  0.1764,  0.1041]]],\n",
      "\n",
      "\n",
      "        [[[-0.1041, -0.1756, -0.0099,  0.1324, -0.0711],\n",
      "          [-0.1375,  0.1822, -0.1874,  0.1650,  0.1747],\n",
      "          [-0.0037,  0.1014, -0.1198, -0.0087,  0.1425],\n",
      "          [ 0.1414, -0.0388, -0.0791,  0.1273,  0.1923],\n",
      "          [-0.1885, -0.1841, -0.0007,  0.1596, -0.0474]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1202, -0.0682,  0.0707, -0.0517, -0.1052],\n",
      "          [-0.0430,  0.0562, -0.0550, -0.1887, -0.1242],\n",
      "          [ 0.0232,  0.1703, -0.0881, -0.1016, -0.0428],\n",
      "          [-0.0526, -0.1067, -0.1118, -0.1638,  0.0234],\n",
      "          [ 0.1164,  0.1088, -0.1650, -0.1457, -0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1187,  0.1789, -0.0574,  0.0419, -0.0685],\n",
      "          [-0.0569, -0.1850, -0.0304,  0.1084, -0.0858],\n",
      "          [ 0.0771,  0.0846, -0.0254,  0.1080, -0.1755],\n",
      "          [-0.0248,  0.1424, -0.0582, -0.1065, -0.1570],\n",
      "          [-0.1464,  0.0791, -0.0423, -0.0227,  0.0283]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0745,  0.0896,  0.0968,  0.1474,  0.0836, -0.0275],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0706,  0.0778,  0.0349,  0.0140,  0.0097],\n",
      "          [ 0.0562, -0.0005,  0.0566,  0.0110,  0.0011],\n",
      "          [ 0.0212,  0.0803,  0.0503,  0.0352, -0.0673],\n",
      "          [ 0.0106,  0.0816, -0.0570,  0.0205, -0.0274],\n",
      "          [ 0.0609, -0.0305,  0.0763,  0.0165,  0.0782]],\n",
      "\n",
      "         [[-0.0728,  0.0090,  0.0629,  0.0665,  0.0483],\n",
      "          [ 0.0752,  0.0402, -0.0409, -0.0322, -0.0414],\n",
      "          [-0.0525, -0.0573,  0.0380,  0.0648, -0.0233],\n",
      "          [ 0.0631, -0.0123,  0.0367, -0.0229, -0.0651],\n",
      "          [ 0.0510,  0.0678,  0.0471, -0.0589, -0.0813]],\n",
      "\n",
      "         [[ 0.0330, -0.0787,  0.0150, -0.0037, -0.0102],\n",
      "          [-0.0449, -0.0556,  0.0661,  0.0782, -0.0673],\n",
      "          [-0.0453, -0.0070, -0.0470,  0.0612,  0.0803],\n",
      "          [-0.0704, -0.0392,  0.0534, -0.0429, -0.0700],\n",
      "          [ 0.0208,  0.0688,  0.0036, -0.0338, -0.0063]],\n",
      "\n",
      "         [[ 0.0435, -0.0211,  0.0334, -0.0161, -0.0796],\n",
      "          [-0.0227, -0.0706,  0.0108,  0.0192, -0.0785],\n",
      "          [-0.0130, -0.0606, -0.0575, -0.0355,  0.0096],\n",
      "          [ 0.0473, -0.0582, -0.0637, -0.0668, -0.0808],\n",
      "          [-0.0240, -0.0190,  0.0792, -0.0016,  0.0530]],\n",
      "\n",
      "         [[ 0.0565, -0.0363,  0.0375,  0.0553,  0.0510],\n",
      "          [ 0.0003,  0.0744,  0.0731, -0.0330, -0.0014],\n",
      "          [ 0.0394, -0.0043, -0.0185,  0.0347,  0.0775],\n",
      "          [ 0.0392,  0.0182, -0.0461, -0.0315,  0.0368],\n",
      "          [-0.0517,  0.0074,  0.0377,  0.0029,  0.0244]],\n",
      "\n",
      "         [[-0.0757,  0.0578,  0.0268, -0.0123,  0.0694],\n",
      "          [-0.0202, -0.0045, -0.0476,  0.0389,  0.0388],\n",
      "          [-0.0349, -0.0162,  0.0028,  0.0467,  0.0004],\n",
      "          [-0.0188,  0.0220,  0.0813,  0.0816, -0.0775],\n",
      "          [-0.0121, -0.0222, -0.0747,  0.0153, -0.0289]]],\n",
      "\n",
      "\n",
      "        [[[-0.0341,  0.0684, -0.0151, -0.0174,  0.0326],\n",
      "          [-0.0816,  0.0234, -0.0594,  0.0382, -0.0402],\n",
      "          [-0.0027, -0.0263, -0.0603, -0.0426,  0.0019],\n",
      "          [-0.0518, -0.0073, -0.0504, -0.0281, -0.0288],\n",
      "          [-0.0056, -0.0736, -0.0576, -0.0170,  0.0068]],\n",
      "\n",
      "         [[ 0.0396, -0.0235, -0.0685, -0.0689,  0.0556],\n",
      "          [ 0.0259,  0.0450, -0.0161, -0.0751,  0.0591],\n",
      "          [-0.0615,  0.0268,  0.0131,  0.0028, -0.0765],\n",
      "          [ 0.0584, -0.0325,  0.0299,  0.0576,  0.0440],\n",
      "          [-0.0452, -0.0106, -0.0303, -0.0517, -0.0628]],\n",
      "\n",
      "         [[-0.0286,  0.0177,  0.0303,  0.0809, -0.0232],\n",
      "          [ 0.0044,  0.0765,  0.0395,  0.0369, -0.0489],\n",
      "          [ 0.0128,  0.0345,  0.0471, -0.0571, -0.0088],\n",
      "          [-0.0040, -0.0539, -0.0533,  0.0253,  0.0314],\n",
      "          [-0.0732, -0.0220,  0.0374, -0.0025, -0.0687]],\n",
      "\n",
      "         [[-0.0326,  0.0318, -0.0037,  0.0609,  0.0169],\n",
      "          [-0.0362, -0.0048,  0.0478,  0.0693,  0.0297],\n",
      "          [ 0.0471, -0.0120, -0.0183, -0.0781, -0.0652],\n",
      "          [ 0.0148, -0.0151, -0.0787, -0.0654,  0.0302],\n",
      "          [ 0.0622, -0.0086,  0.0753,  0.0763, -0.0447]],\n",
      "\n",
      "         [[-0.0737, -0.0477, -0.0742,  0.0656, -0.0530],\n",
      "          [ 0.0735, -0.0104,  0.0372, -0.0557,  0.0123],\n",
      "          [-0.0269,  0.0090,  0.0610, -0.0521,  0.0763],\n",
      "          [ 0.0128,  0.0437,  0.0731,  0.0538, -0.0730],\n",
      "          [-0.0265,  0.0302, -0.0789,  0.0071, -0.0568]],\n",
      "\n",
      "         [[ 0.0375,  0.0296, -0.0697,  0.0162, -0.0382],\n",
      "          [ 0.0385,  0.0075, -0.0118, -0.0170,  0.0340],\n",
      "          [-0.0738,  0.0126, -0.0527, -0.0455, -0.0376],\n",
      "          [ 0.0296,  0.0494,  0.0391, -0.0417,  0.0406],\n",
      "          [-0.0538,  0.0567,  0.0064,  0.0541, -0.0595]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0102,  0.0505,  0.0624, -0.0649,  0.0755],\n",
      "          [-0.0233, -0.0447,  0.0151,  0.0107, -0.0474],\n",
      "          [ 0.0445, -0.0509,  0.0176, -0.0178, -0.0605],\n",
      "          [-0.0545, -0.0711,  0.0464,  0.0644, -0.0039],\n",
      "          [ 0.0112,  0.0263, -0.0735,  0.0691,  0.0742]],\n",
      "\n",
      "         [[ 0.0092, -0.0788,  0.0328,  0.0065,  0.0546],\n",
      "          [ 0.0006,  0.0309, -0.0547,  0.0226, -0.0730],\n",
      "          [-0.0335,  0.0004,  0.0398,  0.0226,  0.0340],\n",
      "          [ 0.0352,  0.0583,  0.0633, -0.0503,  0.0228],\n",
      "          [-0.0518, -0.0648, -0.0542,  0.0424,  0.0302]],\n",
      "\n",
      "         [[-0.0757, -0.0475, -0.0576,  0.0494,  0.0398],\n",
      "          [-0.0153,  0.0189, -0.0150, -0.0374,  0.0308],\n",
      "          [ 0.0511,  0.0478,  0.0607,  0.0690, -0.0191],\n",
      "          [ 0.0123, -0.0134, -0.0255,  0.0749,  0.0427],\n",
      "          [-0.0419,  0.0812,  0.0378, -0.0383,  0.0435]],\n",
      "\n",
      "         [[-0.0475, -0.0245,  0.0160,  0.0200, -0.0797],\n",
      "          [ 0.0439,  0.0182,  0.0214, -0.0397,  0.0183],\n",
      "          [-0.0790,  0.0078,  0.0246,  0.0194, -0.0345],\n",
      "          [ 0.0306, -0.0403,  0.0358, -0.0620, -0.0249],\n",
      "          [-0.0530,  0.0389,  0.0568, -0.0208,  0.0776]],\n",
      "\n",
      "         [[ 0.0384,  0.0252, -0.0730,  0.0004, -0.0569],\n",
      "          [ 0.0639, -0.0585,  0.0496, -0.0580, -0.0647],\n",
      "          [-0.0138,  0.0312,  0.0166,  0.0241,  0.0473],\n",
      "          [ 0.0098,  0.0316,  0.0686,  0.0492, -0.0017],\n",
      "          [ 0.0133, -0.0743,  0.0661,  0.0006, -0.0791]],\n",
      "\n",
      "         [[-0.0448, -0.0453,  0.0480,  0.0600,  0.0251],\n",
      "          [ 0.0712,  0.0094,  0.0271, -0.0621,  0.0394],\n",
      "          [-0.0236,  0.0608,  0.0630, -0.0178,  0.0688],\n",
      "          [-0.0687, -0.0357, -0.0255, -0.0171,  0.0076],\n",
      "          [ 0.0242, -0.0794, -0.0295, -0.0732,  0.0480]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0125, -0.0444,  0.0423, -0.0639,  0.0528],\n",
      "          [ 0.0784,  0.0346, -0.0274,  0.0242,  0.0451],\n",
      "          [-0.0711,  0.0780,  0.0767,  0.0272, -0.0100],\n",
      "          [-0.0799,  0.0454,  0.0162,  0.0372,  0.0454],\n",
      "          [ 0.0642, -0.0741, -0.0222,  0.0146,  0.0053]],\n",
      "\n",
      "         [[-0.0695, -0.0613,  0.0776, -0.0153,  0.0774],\n",
      "          [ 0.0526, -0.0472,  0.0429,  0.0735, -0.0608],\n",
      "          [-0.0046,  0.0655,  0.0478, -0.0745,  0.0581],\n",
      "          [ 0.0612,  0.0124,  0.0366,  0.0379, -0.0273],\n",
      "          [ 0.0084,  0.0316,  0.0667,  0.0804, -0.0403]],\n",
      "\n",
      "         [[ 0.0108, -0.0687, -0.0767, -0.0397, -0.0454],\n",
      "          [ 0.0648,  0.0680,  0.0502,  0.0138,  0.0418],\n",
      "          [ 0.0503,  0.0638,  0.0807, -0.0520,  0.0695],\n",
      "          [-0.0071, -0.0458,  0.0329,  0.0582, -0.0351],\n",
      "          [ 0.0287, -0.0224,  0.0305,  0.0511,  0.0399]],\n",
      "\n",
      "         [[ 0.0443,  0.0319,  0.0451, -0.0206, -0.0752],\n",
      "          [-0.0806, -0.0540,  0.0759, -0.0383, -0.0287],\n",
      "          [-0.0072, -0.0056,  0.0686, -0.0462, -0.0487],\n",
      "          [-0.0455,  0.0569,  0.0780,  0.0338,  0.0227],\n",
      "          [-0.0083,  0.0201,  0.0054, -0.0812, -0.0477]],\n",
      "\n",
      "         [[ 0.0257,  0.0531,  0.0209,  0.0785, -0.0467],\n",
      "          [-0.0067,  0.0487,  0.0300,  0.0603, -0.0694],\n",
      "          [ 0.0170, -0.0185, -0.0156,  0.0380, -0.0251],\n",
      "          [-0.0185,  0.0760, -0.0650,  0.0389,  0.0286],\n",
      "          [ 0.0762,  0.0490, -0.0650, -0.0723, -0.0298]],\n",
      "\n",
      "         [[ 0.0348, -0.0399, -0.0497, -0.0814,  0.0167],\n",
      "          [ 0.0057, -0.0474,  0.0381,  0.0083,  0.0522],\n",
      "          [-0.0599,  0.0052, -0.0402,  0.0097, -0.0674],\n",
      "          [ 0.0795, -0.0309,  0.0773,  0.0200,  0.0682],\n",
      "          [ 0.0466,  0.0284,  0.0497, -0.0018,  0.0747]]],\n",
      "\n",
      "\n",
      "        [[[-0.0074, -0.0078,  0.0183, -0.0421, -0.0392],\n",
      "          [-0.0561, -0.0265, -0.0762, -0.0335, -0.0376],\n",
      "          [-0.0778, -0.0757, -0.0524, -0.0213, -0.0742],\n",
      "          [-0.0340, -0.0698, -0.0488,  0.0540,  0.0164],\n",
      "          [-0.0271,  0.0133,  0.0437,  0.0318, -0.0311]],\n",
      "\n",
      "         [[-0.0065,  0.0104,  0.0441, -0.0012, -0.0162],\n",
      "          [ 0.0471,  0.0625, -0.0392, -0.0049, -0.0211],\n",
      "          [-0.0486, -0.0107, -0.0121,  0.0213, -0.0092],\n",
      "          [-0.0711, -0.0564,  0.0521,  0.0656,  0.0170],\n",
      "          [ 0.0137, -0.0172, -0.0777,  0.0683,  0.0336]],\n",
      "\n",
      "         [[ 0.0756,  0.0044, -0.0257,  0.0788, -0.0567],\n",
      "          [-0.0222,  0.0076, -0.0734,  0.0702,  0.0351],\n",
      "          [ 0.0077,  0.0512,  0.0097,  0.0579,  0.0514],\n",
      "          [-0.0024, -0.0458, -0.0122,  0.0209, -0.0121],\n",
      "          [-0.0500, -0.0305, -0.0545, -0.0117,  0.0549]],\n",
      "\n",
      "         [[ 0.0715, -0.0033,  0.0609, -0.0447, -0.0521],\n",
      "          [-0.0407,  0.0543, -0.0152,  0.0679,  0.0619],\n",
      "          [ 0.0548, -0.0242, -0.0145, -0.0337, -0.0715],\n",
      "          [-0.0604, -0.0440,  0.0262,  0.0753,  0.0606],\n",
      "          [-0.0273,  0.0409, -0.0623, -0.0451,  0.0630]],\n",
      "\n",
      "         [[-0.0086, -0.0391, -0.0220,  0.0049, -0.0513],\n",
      "          [-0.0220,  0.0218,  0.0235, -0.0667,  0.0656],\n",
      "          [-0.0657,  0.0487, -0.0674,  0.0546, -0.0654],\n",
      "          [ 0.0231, -0.0751,  0.0087,  0.0126, -0.0038],\n",
      "          [-0.0650, -0.0624,  0.0393,  0.0779, -0.0716]],\n",
      "\n",
      "         [[-0.0508, -0.0044, -0.0016, -0.0311,  0.0586],\n",
      "          [-0.0516, -0.0687,  0.0023, -0.0292,  0.0386],\n",
      "          [-0.0171,  0.0518,  0.0643,  0.0633, -0.0180],\n",
      "          [-0.0488,  0.0306, -0.0518, -0.0027,  0.0642],\n",
      "          [-0.0001, -0.0527, -0.0673,  0.0620, -0.0763]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0212,  0.0486, -0.0576,  0.0571, -0.0636],\n",
      "          [ 0.0383, -0.0173, -0.0159, -0.0271,  0.0406],\n",
      "          [ 0.0173, -0.0617, -0.0807,  0.0074, -0.0373],\n",
      "          [-0.0187,  0.0523,  0.0034,  0.0513, -0.0112],\n",
      "          [-0.0725,  0.0769, -0.0255,  0.0521, -0.0449]],\n",
      "\n",
      "         [[ 0.0799, -0.0501, -0.0018, -0.0719, -0.0725],\n",
      "          [-0.0449,  0.0490, -0.0450,  0.0131, -0.0643],\n",
      "          [-0.0193,  0.0733,  0.0188, -0.0432,  0.0467],\n",
      "          [-0.0161,  0.0608,  0.0495,  0.0142,  0.0127],\n",
      "          [ 0.0519, -0.0668,  0.0540, -0.0449,  0.0583]],\n",
      "\n",
      "         [[-0.0122,  0.0099,  0.0594,  0.0563, -0.0471],\n",
      "          [ 0.0207,  0.0193,  0.0015, -0.0741,  0.0601],\n",
      "          [ 0.0697,  0.0736, -0.0076, -0.0583,  0.0344],\n",
      "          [-0.0273, -0.0710, -0.0470, -0.0433, -0.0245],\n",
      "          [-0.0205,  0.0311,  0.0024,  0.0349,  0.0211]],\n",
      "\n",
      "         [[-0.0036,  0.0812,  0.0195,  0.0680, -0.0113],\n",
      "          [-0.0618, -0.0144, -0.0261,  0.0625, -0.0333],\n",
      "          [-0.0361, -0.0033, -0.0112,  0.0185, -0.0723],\n",
      "          [ 0.0311, -0.0332, -0.0065, -0.0222,  0.0330],\n",
      "          [-0.0791,  0.0666, -0.0627,  0.0034, -0.0663]],\n",
      "\n",
      "         [[-0.0807,  0.0178, -0.0739, -0.0307,  0.0448],\n",
      "          [ 0.0232,  0.0735,  0.0462,  0.0673,  0.0748],\n",
      "          [-0.0035, -0.0700,  0.0551, -0.0811,  0.0352],\n",
      "          [ 0.0316,  0.0632,  0.0351, -0.0580, -0.0232],\n",
      "          [-0.0210,  0.0148,  0.0693, -0.0486, -0.0484]],\n",
      "\n",
      "         [[-0.0432, -0.0605,  0.0217,  0.0007, -0.0102],\n",
      "          [ 0.0329, -0.0627,  0.0431, -0.0621,  0.0492],\n",
      "          [-0.0360,  0.0733, -0.0250, -0.0073,  0.0297],\n",
      "          [ 0.0718, -0.0545,  0.0207, -0.0519,  0.0686],\n",
      "          [ 0.0735,  0.0227,  0.0735,  0.0103, -0.0314]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0466, -0.0263, -0.0290, -0.0534, -0.0376,  0.0471,  0.0599, -0.0207,\n",
      "         0.0247, -0.0482, -0.0264,  0.0052,  0.0529,  0.0684, -0.0386, -0.0585],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0491, -0.0177,  0.0047,  ..., -0.0041,  0.0092, -0.0276],\n",
      "        [-0.0148, -0.0398, -0.0075,  ..., -0.0421, -0.0430,  0.0057],\n",
      "        [ 0.0141, -0.0097, -0.0325,  ..., -0.0339,  0.0153, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0432, -0.0291,  ..., -0.0345,  0.0437,  0.0117],\n",
      "        [-0.0032,  0.0029,  0.0471,  ...,  0.0093,  0.0328, -0.0226],\n",
      "        [-0.0032, -0.0294,  0.0060,  ...,  0.0300, -0.0075,  0.0300]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0301,  0.0211,  0.0254,  0.0475, -0.0342, -0.0280, -0.0164, -0.0192,\n",
      "         0.0374,  0.0091,  0.0183, -0.0206, -0.0416,  0.0169, -0.0174,  0.0065,\n",
      "        -0.0456, -0.0345,  0.0483, -0.0013, -0.0216, -0.0300, -0.0280, -0.0114,\n",
      "         0.0031, -0.0369, -0.0413, -0.0131, -0.0261,  0.0203,  0.0391, -0.0095,\n",
      "        -0.0224, -0.0328,  0.0206,  0.0117, -0.0291,  0.0421,  0.0039,  0.0067,\n",
      "         0.0267, -0.0101, -0.0018,  0.0379,  0.0447,  0.0157,  0.0374, -0.0309,\n",
      "         0.0079,  0.0067,  0.0126,  0.0386, -0.0408, -0.0112,  0.0480, -0.0041,\n",
      "        -0.0488,  0.0075,  0.0432, -0.0248,  0.0320,  0.0325, -0.0142,  0.0083,\n",
      "        -0.0052, -0.0235,  0.0113,  0.0393, -0.0051, -0.0367, -0.0127,  0.0054,\n",
      "        -0.0372, -0.0354,  0.0418,  0.0389,  0.0297,  0.0006,  0.0053, -0.0355,\n",
      "         0.0040, -0.0253, -0.0073, -0.0410, -0.0389,  0.0243, -0.0062, -0.0249,\n",
      "        -0.0013,  0.0108,  0.0401, -0.0114,  0.0225, -0.0356, -0.0051, -0.0441,\n",
      "         0.0045, -0.0366, -0.0455,  0.0448, -0.0035, -0.0033,  0.0166, -0.0459,\n",
      "        -0.0466, -0.0124, -0.0141,  0.0488,  0.0499, -0.0186, -0.0165, -0.0189,\n",
      "        -0.0086,  0.0148,  0.0450,  0.0359, -0.0166, -0.0486,  0.0018, -0.0256],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0867, -0.0027, -0.0662,  ..., -0.0618, -0.0474, -0.0095],\n",
      "        [-0.0637, -0.0277,  0.0704,  ..., -0.0864, -0.0455, -0.0255],\n",
      "        [ 0.0727,  0.0112,  0.0668,  ...,  0.0132,  0.0825,  0.0486],\n",
      "        ...,\n",
      "        [-0.0671, -0.0910, -0.0453,  ...,  0.0487,  0.0050, -0.0528],\n",
      "        [ 0.0026,  0.0413,  0.0246,  ...,  0.0340, -0.0694,  0.0582],\n",
      "        [ 0.0318,  0.0649,  0.0896,  ...,  0.0711, -0.0043, -0.0301]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0533,  0.0828, -0.0311,  0.0720, -0.0068,  0.0591, -0.0282,  0.0701,\n",
      "         0.0522,  0.0006, -0.0844,  0.0603,  0.0891,  0.0655,  0.0726, -0.0889,\n",
      "        -0.0010,  0.0327,  0.0027, -0.0200,  0.0289,  0.0101, -0.0427, -0.0496,\n",
      "         0.0267, -0.0606, -0.0417, -0.0694,  0.0210, -0.0524, -0.0129,  0.0509,\n",
      "         0.0183,  0.0468, -0.0280, -0.0857, -0.0451,  0.0748,  0.0325, -0.0172,\n",
      "         0.0180,  0.0791,  0.0811,  0.0635,  0.0305,  0.0112,  0.0431, -0.0835,\n",
      "         0.0665,  0.0511, -0.0301, -0.0165, -0.0670,  0.0666,  0.0290, -0.0155,\n",
      "         0.0152,  0.0087, -0.0893,  0.0508, -0.0231,  0.0421,  0.0050,  0.0648,\n",
      "        -0.0268, -0.0047,  0.0848, -0.0589, -0.0009,  0.0135,  0.0057, -0.0660,\n",
      "         0.0402, -0.0484,  0.0457, -0.0720,  0.0298, -0.0842,  0.0825, -0.0375,\n",
      "        -0.0178, -0.0383, -0.0646,  0.0004], requires_grad=True), Parameter containing:\n",
      "tensor([[ 9.3091e-02, -1.4230e-03, -4.7095e-02,  6.9285e-02, -5.3969e-02,\n",
      "          4.3363e-04, -1.0626e-01, -3.0223e-02, -9.0766e-02, -3.5883e-02,\n",
      "         -3.1249e-02,  3.3167e-02,  6.6159e-02,  1.8354e-02,  6.3116e-02,\n",
      "         -3.9629e-02, -3.8480e-02, -6.2526e-02,  4.6287e-02, -6.0412e-02,\n",
      "          8.7376e-02, -7.0109e-02, -2.7930e-03,  8.6682e-02, -6.0505e-02,\n",
      "          6.0319e-02, -1.8006e-02, -7.3227e-02,  3.5753e-02,  1.7733e-02,\n",
      "         -9.9288e-02, -3.8096e-02,  8.5106e-02, -5.5067e-02, -8.6498e-02,\n",
      "         -9.5629e-02, -2.8709e-03,  1.1024e-02,  1.9249e-03,  3.9869e-02,\n",
      "          2.9073e-02, -7.0170e-02,  2.7912e-02,  7.3231e-02, -3.4559e-02,\n",
      "          5.7438e-02, -9.8550e-02, -9.0303e-02,  4.0627e-04,  5.7325e-03,\n",
      "         -8.6561e-02,  6.5071e-02, -8.7998e-02, -4.7179e-02, -4.4546e-02,\n",
      "         -2.2746e-02, -7.0443e-03, -2.1455e-02,  1.2241e-02, -7.0949e-02,\n",
      "         -3.5919e-02,  4.2696e-02,  1.0812e-01,  8.4779e-02, -1.3941e-02,\n",
      "         -8.4183e-02,  3.4953e-02,  5.3994e-02,  6.2133e-02, -4.5715e-02,\n",
      "         -4.4264e-02, -6.0598e-02,  8.6386e-03, -6.9461e-02,  7.4557e-02,\n",
      "         -3.8510e-02, -6.2974e-02,  3.5482e-02, -1.7416e-02, -3.6436e-02,\n",
      "         -2.3927e-02,  7.2117e-02,  1.0340e-01,  5.1127e-02],\n",
      "        [ 1.0434e-01, -1.8439e-02, -9.8326e-02, -1.0050e-01,  7.1289e-02,\n",
      "         -4.3023e-02,  5.3960e-03,  6.5264e-02,  1.0320e-01,  4.4720e-02,\n",
      "          9.9565e-02, -3.5020e-02, -1.0856e-01, -6.5483e-02,  4.0457e-03,\n",
      "          7.5230e-02, -4.8047e-02, -2.0382e-02,  8.7282e-03, -5.3598e-02,\n",
      "         -2.1296e-02,  6.9650e-02,  1.5676e-02, -5.5490e-02, -1.0413e-01,\n",
      "          8.1201e-02, -3.9640e-02, -8.6902e-02,  7.1982e-02, -4.3979e-02,\n",
      "          5.7718e-02, -9.1281e-02,  1.8039e-02,  1.8073e-02,  3.4470e-02,\n",
      "         -5.2307e-02,  7.7576e-02,  9.0237e-02,  5.6077e-02, -1.2179e-02,\n",
      "          3.0562e-02, -1.8701e-02,  5.1095e-02, -5.9180e-03,  4.5773e-02,\n",
      "          1.0476e-01, -1.0232e-01, -1.8355e-02,  5.0513e-02, -1.8217e-02,\n",
      "         -1.0577e-01, -8.5980e-02, -4.1255e-02, -1.8257e-02,  5.2967e-02,\n",
      "         -3.1759e-02, -6.9087e-02, -6.1672e-02,  4.6489e-02,  6.3062e-02,\n",
      "          5.8601e-02, -9.1582e-03, -5.6216e-02,  2.2624e-02,  4.3012e-02,\n",
      "          6.2221e-02,  6.5005e-02,  5.7431e-02, -2.6785e-03,  1.0638e-01,\n",
      "          3.3338e-03,  9.6300e-02,  3.8852e-02, -6.3333e-02, -9.9253e-02,\n",
      "         -8.9931e-02,  1.0115e-02, -7.3501e-02, -8.5533e-02,  6.7365e-02,\n",
      "         -7.6432e-02,  9.7845e-02, -5.4730e-02, -1.0848e-01],\n",
      "        [-9.9953e-02, -4.3765e-02,  3.5108e-02,  4.9415e-02, -3.1456e-02,\n",
      "          7.4720e-02,  7.5239e-03,  1.4277e-02,  5.7466e-02,  5.5259e-02,\n",
      "          1.0721e-01, -5.6332e-02,  2.8540e-02,  9.6742e-02,  6.8734e-02,\n",
      "         -1.0295e-01, -1.4602e-02, -9.1009e-02,  6.2042e-02, -7.8296e-02,\n",
      "          3.0242e-03, -8.1130e-02,  1.7162e-02, -2.6384e-02,  9.6531e-02,\n",
      "         -5.9619e-02, -2.0565e-02,  5.8370e-03, -7.1360e-02, -4.4684e-03,\n",
      "         -1.9029e-02,  9.6334e-02,  1.4164e-02,  2.9515e-02, -6.5604e-02,\n",
      "         -5.5351e-02,  3.6999e-02,  1.0395e-01,  7.2567e-02, -2.7158e-02,\n",
      "         -2.4947e-02, -9.4538e-02,  8.6679e-02, -6.9980e-02, -3.4789e-02,\n",
      "          6.0611e-02, -2.5717e-02,  1.0158e-01, -1.0023e-01,  1.8399e-02,\n",
      "         -3.9817e-02, -1.0470e-01, -3.7308e-02, -1.9496e-02, -6.6347e-02,\n",
      "          1.0798e-01,  2.9324e-02,  6.1847e-02,  1.0100e-01,  7.3113e-02,\n",
      "         -3.1627e-03,  4.1216e-03, -7.2247e-02, -5.1726e-03, -6.9330e-02,\n",
      "          8.5912e-02, -9.2102e-02,  5.4637e-02,  5.3331e-04, -2.0048e-02,\n",
      "         -3.5154e-02,  1.0528e-01, -9.3108e-02,  9.5999e-02,  5.7835e-02,\n",
      "          4.0468e-02,  4.0897e-02,  6.6598e-02,  3.8088e-02, -2.1616e-02,\n",
      "          2.5899e-02,  3.7996e-02, -1.9637e-02, -8.7388e-02],\n",
      "        [-2.7335e-02,  7.2586e-02,  1.0468e-01,  2.3906e-02, -4.7970e-02,\n",
      "          1.0426e-02,  6.7113e-03,  1.6362e-02, -6.0987e-02, -4.2270e-02,\n",
      "         -3.1791e-02,  1.0610e-01, -4.5084e-02, -4.8701e-02, -1.0074e-01,\n",
      "          3.5794e-02,  4.0719e-02,  3.9589e-02, -8.3866e-02,  5.7432e-03,\n",
      "          1.9006e-02, -9.1388e-02,  7.6294e-02,  5.2465e-02,  1.4386e-02,\n",
      "         -1.0864e-01,  2.1073e-02,  9.1935e-02,  5.7974e-02,  2.5038e-02,\n",
      "         -6.8844e-03, -5.8485e-03,  9.4099e-02,  4.7829e-02, -3.0187e-02,\n",
      "         -4.9268e-02, -1.0060e-01, -7.2925e-02,  3.2672e-02, -2.8110e-03,\n",
      "         -5.9277e-02, -1.5779e-02, -1.4195e-02, -4.6710e-02, -2.0222e-02,\n",
      "          7.6441e-02, -7.4749e-02, -5.5367e-02, -2.5855e-02,  8.0306e-02,\n",
      "          5.1117e-02,  8.2313e-02,  2.1157e-02, -5.1871e-02, -4.1597e-02,\n",
      "         -1.0888e-01,  8.7653e-02,  7.7815e-03,  1.3143e-02,  9.0956e-02,\n",
      "         -6.3613e-02, -1.6271e-02, -2.2666e-02,  5.8071e-02,  9.9609e-02,\n",
      "          5.5076e-02, -9.4018e-02,  3.4402e-02,  7.6446e-03,  5.6142e-02,\n",
      "          3.6592e-02,  4.7379e-02,  5.5199e-02, -1.1843e-02,  4.0991e-02,\n",
      "          4.5148e-02,  1.9247e-02, -4.5400e-02, -7.0827e-02,  7.2242e-02,\n",
      "          3.5084e-02,  4.1912e-02,  8.1854e-02, -5.9180e-02],\n",
      "        [-1.0167e-01, -8.3477e-02,  7.2281e-02, -1.2315e-02,  8.3972e-02,\n",
      "         -8.1443e-02, -8.3915e-02, -2.2903e-02,  5.0469e-02,  2.7670e-02,\n",
      "         -6.9366e-02,  1.1403e-02, -2.5425e-02, -7.8757e-02, -5.0498e-02,\n",
      "         -6.8909e-02, -1.0291e-01, -3.1099e-02,  6.4815e-02, -8.5659e-02,\n",
      "         -1.0664e-01,  6.7665e-02, -7.2581e-03, -5.8530e-03, -1.3021e-02,\n",
      "          7.1773e-02, -7.0913e-02, -1.0209e-01, -9.6939e-02, -2.5079e-02,\n",
      "          7.4818e-02, -2.3413e-02, -8.7647e-02, -1.5464e-02,  5.2357e-02,\n",
      "         -1.8052e-02,  8.4971e-02, -9.9123e-02, -5.0952e-02,  3.2583e-02,\n",
      "         -6.7528e-02, -1.0716e-01,  1.5303e-02, -1.4503e-02,  4.9828e-02,\n",
      "         -5.4133e-02,  1.0490e-01,  1.7149e-03, -8.9407e-08, -1.8112e-03,\n",
      "          5.8908e-02, -6.4749e-02, -1.0395e-01, -6.2160e-03, -9.0685e-02,\n",
      "         -8.8931e-02, -7.5489e-02,  6.2651e-02, -2.9653e-02,  3.3848e-02,\n",
      "          9.2703e-02, -7.6912e-02,  6.0386e-02,  7.3830e-03,  3.0081e-02,\n",
      "          5.6831e-02,  7.0704e-02,  1.2247e-02, -5.6443e-02, -7.1087e-02,\n",
      "          6.4004e-02,  3.1505e-02,  1.1159e-02, -3.5119e-02,  4.5558e-02,\n",
      "          6.3133e-02, -9.0649e-02, -8.4506e-02, -7.1358e-03, -1.0833e-01,\n",
      "         -1.7089e-02, -8.7908e-03,  8.3100e-02, -2.4391e-02],\n",
      "        [-1.0126e-01,  1.0764e-01,  8.5355e-03,  1.0857e-01,  7.9958e-02,\n",
      "         -5.7413e-02,  3.0314e-02, -4.9279e-02, -4.1571e-03,  7.2212e-02,\n",
      "          3.0862e-02, -8.6787e-02, -2.5883e-03, -4.1713e-02,  8.2105e-02,\n",
      "          7.0040e-02,  9.4199e-02,  4.2629e-02, -6.7900e-02,  7.0574e-02,\n",
      "         -7.3568e-02,  6.1880e-02,  6.7385e-02,  8.0892e-02, -2.8625e-03,\n",
      "         -6.6638e-02, -7.7489e-02, -9.8147e-03, -2.6045e-02, -3.9512e-02,\n",
      "         -2.7764e-02, -5.8494e-02,  1.0590e-01,  1.1550e-02,  5.4861e-02,\n",
      "         -5.3650e-02, -8.4044e-03,  7.4253e-02, -2.0000e-02, -1.0356e-01,\n",
      "         -7.5724e-02,  1.4512e-03,  1.0334e-01,  2.5458e-02, -7.8422e-02,\n",
      "          1.8169e-02,  7.6911e-03, -6.9728e-02, -1.0368e-01,  1.0330e-01,\n",
      "         -9.8462e-02,  2.5421e-02, -2.3495e-02, -9.5831e-02,  8.6509e-02,\n",
      "         -6.5331e-02,  6.2300e-02, -6.1674e-03,  6.3114e-02,  1.0703e-01,\n",
      "         -9.8755e-02,  4.7384e-03,  8.0018e-03, -7.5012e-02, -9.2674e-02,\n",
      "         -5.2834e-02, -7.4150e-03, -1.0578e-01, -1.0693e-01, -7.5050e-02,\n",
      "         -2.8421e-02,  6.0431e-02, -5.4147e-02, -1.0397e-01,  7.3474e-02,\n",
      "          9.3447e-02, -7.1759e-02, -3.6882e-02,  8.5601e-02,  4.9735e-02,\n",
      "         -8.3365e-02,  6.0634e-02,  8.9468e-02,  8.7817e-02],\n",
      "        [ 1.0510e-01,  2.3714e-02,  5.1395e-02,  9.9591e-02,  7.2563e-02,\n",
      "          5.5449e-02,  9.2310e-02,  1.0878e-01, -7.0190e-02,  6.8494e-02,\n",
      "         -7.1046e-02,  3.7360e-02,  4.8809e-02,  5.7280e-02, -3.7744e-02,\n",
      "         -1.4627e-02,  7.3173e-02,  9.4195e-02, -6.0237e-02, -3.3068e-02,\n",
      "         -1.9642e-02,  5.4306e-02, -9.9334e-03, -7.1867e-02, -1.0855e-01,\n",
      "          5.8581e-02,  5.8535e-02, -6.4517e-02,  2.8712e-02, -5.3353e-02,\n",
      "          7.8234e-02,  4.9988e-03, -1.0423e-01,  8.0918e-02,  4.2957e-02,\n",
      "         -1.0779e-01, -5.8459e-02, -6.5566e-03, -3.3263e-02,  7.7809e-02,\n",
      "         -1.0017e-02, -1.0463e-01, -5.4628e-02,  4.7417e-02,  2.3279e-02,\n",
      "         -8.9538e-02, -9.2221e-02,  3.7964e-02,  3.3347e-02, -6.5333e-02,\n",
      "         -5.0979e-02,  8.1337e-02, -2.8630e-02,  5.1116e-02, -3.9016e-02,\n",
      "          3.4917e-02,  1.0185e-01, -7.9663e-02,  6.0264e-02, -6.1085e-02,\n",
      "         -4.3395e-02,  7.0285e-02,  1.2475e-02, -7.0847e-02,  6.1679e-02,\n",
      "          1.0689e-01,  4.3463e-03,  1.0318e-01, -1.0269e-01,  3.6526e-02,\n",
      "         -1.4152e-02,  2.8083e-02, -6.1311e-02,  4.5104e-02,  1.4104e-02,\n",
      "          9.0401e-02, -7.9298e-03,  5.9729e-02, -1.8554e-02, -1.8901e-03,\n",
      "          2.9778e-02,  2.9314e-02,  5.6841e-02,  1.0663e-01],\n",
      "        [ 7.7826e-02,  1.0077e-01, -6.1119e-02,  9.4511e-02, -2.6082e-02,\n",
      "         -2.1694e-02, -4.5872e-02, -1.3160e-02,  3.3147e-02, -2.4582e-02,\n",
      "          2.2856e-02, -8.8418e-02,  5.4706e-02,  9.5941e-02,  5.4521e-02,\n",
      "          7.6798e-02, -1.0080e-01, -2.9274e-02,  3.1830e-02,  3.9789e-02,\n",
      "          2.0742e-02,  5.5705e-02,  4.6279e-02, -2.5686e-02, -1.0252e-01,\n",
      "         -1.0178e-02, -6.3183e-02, -8.7426e-02, -5.9246e-02,  3.5936e-02,\n",
      "         -1.0559e-01,  3.9577e-02,  9.8321e-02,  1.4041e-02,  8.2932e-02,\n",
      "         -2.0969e-02,  8.3823e-02,  1.0368e-01,  7.9184e-02, -4.9969e-02,\n",
      "          7.0862e-02,  2.7737e-02,  6.3096e-02,  6.1552e-02, -7.6654e-02,\n",
      "         -2.4232e-02, -9.1509e-02,  7.0660e-02,  9.5942e-03, -3.3496e-02,\n",
      "          4.7860e-02,  4.9277e-02,  4.9255e-02, -5.6128e-02,  3.6099e-02,\n",
      "          7.4905e-02, -8.1569e-02,  1.6102e-02, -9.8324e-02,  4.6907e-02,\n",
      "         -2.7501e-02,  8.0362e-03,  1.7051e-02, -1.0728e-01, -4.7588e-02,\n",
      "          4.7384e-02,  1.0513e-01, -8.3731e-02, -5.5317e-02,  9.3011e-02,\n",
      "          4.9706e-03, -4.5119e-02, -4.5983e-02, -3.6670e-03,  9.5666e-02,\n",
      "          1.5899e-02,  3.2809e-02, -4.7843e-02, -2.5085e-03,  3.2352e-02,\n",
      "          6.4692e-02, -1.0099e-01,  7.1168e-03, -1.7884e-02],\n",
      "        [ 9.0673e-02,  4.2115e-02,  6.7864e-02, -4.9264e-02,  8.0936e-02,\n",
      "         -4.5297e-02,  9.5247e-03, -3.5359e-02, -7.2312e-02, -7.4004e-03,\n",
      "          1.6798e-02,  7.6468e-02, -9.5412e-02,  4.5451e-02, -9.5020e-02,\n",
      "         -1.0496e-01, -2.4464e-02, -6.3532e-02, -1.4794e-02, -3.8645e-03,\n",
      "          3.0204e-02, -9.2564e-02,  2.4957e-02,  7.1368e-02, -2.5925e-02,\n",
      "          3.4138e-02,  1.9948e-02, -4.5290e-02,  7.7137e-02, -1.0157e-01,\n",
      "          7.2348e-02, -2.4173e-02, -9.4861e-03, -6.8257e-03,  8.6734e-02,\n",
      "         -6.0123e-02, -7.7161e-02, -2.6823e-02, -6.9312e-02, -4.1290e-02,\n",
      "         -6.3435e-02, -4.3208e-02, -5.0740e-02,  2.7745e-03,  8.5654e-03,\n",
      "         -6.8604e-03, -7.3128e-02,  3.8753e-02,  1.0266e-01,  5.2224e-02,\n",
      "         -4.6047e-02, -5.4775e-02, -3.5772e-02, -1.0316e-01,  5.3053e-02,\n",
      "         -9.4060e-02,  1.0009e-01,  2.6520e-02, -9.4150e-02,  2.9100e-02,\n",
      "          5.8639e-02,  3.0837e-02,  5.7666e-02,  6.5902e-02, -2.8204e-02,\n",
      "          7.6371e-02, -5.7895e-02, -7.9744e-02, -4.4308e-03, -2.1861e-02,\n",
      "          1.7464e-02,  6.7347e-02,  3.2647e-02,  4.5636e-02, -9.4405e-02,\n",
      "          1.1027e-02, -7.5967e-03,  7.0038e-02, -1.1039e-02,  5.4604e-02,\n",
      "         -8.7360e-02,  3.4151e-02,  8.2429e-02, -1.0455e-01],\n",
      "        [ 9.0147e-02, -1.0374e-01, -9.7728e-02, -9.6701e-02,  4.5096e-02,\n",
      "         -5.9066e-02,  4.9667e-02,  9.5948e-02, -3.2623e-02, -9.4915e-02,\n",
      "         -7.5351e-02, -2.8638e-03,  7.3621e-02, -4.7502e-02,  8.6450e-02,\n",
      "          3.4614e-02,  4.8575e-02, -2.6096e-02, -7.1405e-02, -7.1712e-02,\n",
      "         -4.1411e-02, -1.0092e-01,  2.9661e-02,  7.3466e-02, -5.0384e-02,\n",
      "         -1.0576e-02,  3.3971e-02,  6.5901e-02,  1.0843e-01,  1.7976e-02,\n",
      "          9.3146e-02, -6.3610e-02, -4.3745e-02,  4.5050e-02, -9.8260e-02,\n",
      "          5.4738e-02, -2.7891e-02,  1.3497e-02, -9.7603e-02,  6.5034e-02,\n",
      "          1.5591e-02, -5.6944e-02,  8.7370e-02, -9.7147e-02, -7.9323e-02,\n",
      "         -5.9524e-02, -1.6431e-02,  5.1755e-02, -6.8231e-02, -1.4244e-02,\n",
      "         -5.7558e-02, -4.1464e-02,  8.9471e-02, -1.6615e-02, -1.0943e-02,\n",
      "         -6.8767e-02, -4.5052e-02,  7.6250e-02, -4.0274e-02,  6.8610e-02,\n",
      "          9.9028e-02,  8.4817e-02, -4.0193e-02,  1.0869e-01, -4.2687e-02,\n",
      "          8.9832e-02, -1.0371e-01, -6.6551e-02, -1.0560e-01, -6.0172e-02,\n",
      "          7.0875e-02, -5.3846e-02,  1.4071e-02,  1.0610e-02, -7.3747e-02,\n",
      "          7.0544e-02,  8.5925e-02, -6.7364e-02, -1.0475e-01, -6.9107e-02,\n",
      "          1.1864e-02,  6.1604e-02, -1.0733e-01, -8.8227e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0612, -0.0179, -0.1003, -0.0559, -0.0521, -0.0021,  0.0026,  0.0918,\n",
      "         0.0036, -0.0879], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())# conv1 weight\n",
    "print(params[1].size())\n",
    "print(params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 1.3786, -0.1685, -1.5023,  ...,  0.1058, -1.8240, -0.1033],\n          [ 0.8130, -1.4512,  0.5194,  ..., -0.4201,  0.1843, -0.4848],\n          [ 0.4310,  1.5333,  1.6982,  ..., -0.4078, -0.4040,  1.1970],\n          ...,\n          [ 1.0566,  0.6966,  1.5472,  ...,  1.0282,  0.0402, -2.0220],\n          [-1.2033, -1.2452, -0.2053,  ...,  0.6585, -0.5915,  0.7812],\n          [ 1.6567, -1.8875,  0.2209,  ...,  1.1646,  1.6353,  1.2260]]]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 32, 32])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0851,  0.0041, -0.1146, -0.0295, -0.1480,  0.0410,  0.0484,  0.2101,\n",
      "         -0.0708, -0.1421]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = net(input)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将所有参数的梯度缓存清零，然后进行随机梯度的反向传播"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``torch.nn`` 只支持小批量输入。整个 ``torch.nn`` 包都只支持小批量样本，而不支持单个样本。\n",
    "例如，``nn.Conv2d`` 接受一个4维的张量，\n",
    "``每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）``。\n",
    "\n",
    "如果你有单个样本，只需使用 ``input.unsqueeze(0)`` 来添加其它的维数\n",
    "\n",
    "在继续之前，我们回顾一下到目前为止用到的类。\n",
    "\n",
    "回顾:\n",
    "\n",
    "torch.Tensor：一个用过自动调用 backward()实现支持自动梯度计算的 多维数组 ， 并且保存关于这个向量的梯度 w.r.t.\n",
    "nn.Module：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。\n",
    "nn.Parameter：一种变量，当把它赋值给一个Module时，被 自动 地注册为一个参数。\n",
    "autograd.Function：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个Tensor的操作都回创建一个接到创建Tensor和 编码其历史 的函数的Function节点。\n",
    "重点如下：\n",
    "\n",
    "定义一个网络\n",
    "处理输入，调用backword\n",
    "还剩：\n",
    "\n",
    "计算损失\n",
    "更新网络权重\n",
    "损失函数\n",
    "一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。\n",
    "\n",
    "*译者注：output为网络的输出，target为实际值*\n",
    "\n",
    "nn包中有很多不同的损失函数。 nn.MSELoss是一个比较简单的损失函数，它计算输出和目标间的均方误差， 例如："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.6502, grad_fn=<MseLossBackward0>)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) #随机值作为样例\n",
    "target = target.view(1,-1)#使target和output的shape相同\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，如果在反向过程中跟随loss ， 使用它的 .grad_fn 属性，将看到如下所示的计算图。\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "      所以，当我们调用 loss.backward()时,整张计算图都会 根据loss进行微分，而且图中所有设置为requires_grad=True的张量 将会拥有一个随着梯度累积的.grad 张量。\n",
    "\n",
    "为了说明，让我们向后退几步:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x000002452F9170D0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)#MSEloss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddmmBackward0 object at 0x000002452F916F80>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0])#Linear"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AccumulateGrad object at 0x000002452F9179A0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])#Relu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "反向传播\n",
    "调用loss.backward()获得反向传播的误差。\n",
    "\n",
    "但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。\n",
    "\n",
    "现在，我们将调用loss.backward()，并查看conv1层的偏差（bias）项在反向传播前后的梯度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad berore backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad berore backward\n",
      "tensor([-0.0131, -0.0065, -0.0041,  0.0129,  0.0012,  0.0019])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad berore backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('conv1.bias.grad berore backward')\n",
    "print(net.conv1.bias.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "更新权重\n",
    "在实践中最简单的权重更新规则是随机梯度下降（SGD）：\n",
    "\n",
    " ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "我们可以使用简单的Python代码实现这个规则：\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包torch.optim实现了所有的这些规则。 使用它们非常简单："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "#in your training loop\n",
    "optimizer.zero_grad() #zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()# Does the update\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ".. 注意::\n",
    "\n",
    "  观察如何使用``optimizer.zero_grad()``手动将梯度缓冲区设置为零。\n",
    "  这是因为梯度是按Backprop部分中的说明累积的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
